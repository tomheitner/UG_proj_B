{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ver30_09.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RPbB7s8hPHfO",
        "vkTSG22ePKkz",
        "HngQNsyjXmvF",
        "-CC4jbZBiRMg",
        "uwrFI4C9Vvam",
        "YtXyXw6EaKRj",
        "FXLCyVgZvLDl",
        "9Tf3pBu91x4G",
        "cjHuO49maf6R",
        "lLJqU56RdgjV",
        "MwJpRS9_mNdo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnSHZ7OviFm"
      },
      "source": [
        "#Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL_0tSC67biu"
      },
      "source": [
        "<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Use a free Cloud TPU</h3>\n",
        " \n",
        "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "   2. Click Runtime again and select **Runtime > Run All**. You can also run the cells manually with Shift-ENTER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_So6uv8b6Ck"
      },
      "source": [
        "## Mounting Our Google Drive\n",
        "We're ising our Google Drive to store videos and saved models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYdSt6W8b5LT",
        "outputId": "8b1003da-95f4-44fa-8250-6ecf9b59ffcc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODxpKwkNFrBk"
      },
      "source": [
        "## Download the source code\n",
        "Download the source code of the Mask R-CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUaCGBUQFgiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcaf478-7442-4c2b-a1bb-afcafb0b5d22"
      },
      "source": [
        "!git clone https://github.com/tensorflow/tpu/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tpu' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPbB7s8hPHfO"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EtGbyNc8VgS"
      },
      "source": [
        "from IPython import display\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.insert(0, 'tpu/models/official')\n",
        "sys.path.insert(0, 'tpu/models/official/mask_rcnn')\n",
        "import coco_metric\n",
        "from mask_rcnn.object_detection import visualization_utils\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "import codecs\n",
        "import time  # to check frames per second\n",
        "\n",
        "# DINO\n",
        "import torch\n",
        "import torchvision\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkTSG22ePKkz"
      },
      "source": [
        "## Load the COCO index mapping\n",
        "This Colab uses a pretrained checkpoint of the Mask R-CNN model that is trained using the COCO dataset. Here is the mapping between the indices that the model predicts and the categories in text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q5r1zob93OF"
      },
      "source": [
        "ID_MAPPING = {\n",
        "    1: 'person',\n",
        "    2: 'bicycle',\n",
        "    3: 'car',\n",
        "    4: 'motorcycle',\n",
        "    5: 'airplane',\n",
        "    6: 'bus',\n",
        "    7: 'train',\n",
        "    8: 'truck',\n",
        "    9: 'boat',\n",
        "    10: 'traffic light',\n",
        "    11: 'fire hydrant',\n",
        "    13: 'stop sign',\n",
        "    14: 'parking meter',\n",
        "    15: 'bench',\n",
        "    16: 'bird',\n",
        "    17: 'cat',\n",
        "    18: 'dog',\n",
        "    19: 'horse',\n",
        "    20: 'sheep',\n",
        "    21: 'cow',\n",
        "    22: 'elephant',\n",
        "    23: 'bear',\n",
        "    24: 'zebra',\n",
        "    25: 'giraffe',\n",
        "    27: 'backpack',\n",
        "    28: 'umbrella',\n",
        "    31: 'handbag',\n",
        "    32: 'tie',\n",
        "    33: 'suitcase',\n",
        "    34: 'frisbee',\n",
        "    35: 'skis',\n",
        "    36: 'snowboard',\n",
        "    37: 'sports ball',\n",
        "    38: 'kite',\n",
        "    39: 'baseball bat',\n",
        "    40: 'baseball glove',\n",
        "    41: 'skateboard',\n",
        "    42: 'surfboard',\n",
        "    43: 'tennis racket',\n",
        "    44: 'bottle',\n",
        "    46: 'wine glass',\n",
        "    47: 'cup',\n",
        "    48: 'fork',\n",
        "    49: 'knife',\n",
        "    50: 'spoon',\n",
        "    51: 'bowl',\n",
        "    52: 'banana',\n",
        "    53: 'apple',\n",
        "    54: 'sandwich',\n",
        "    55: 'orange',\n",
        "    56: 'broccoli',\n",
        "    57: 'carrot',\n",
        "    58: 'hot dog',\n",
        "    59: 'pizza',\n",
        "    60: 'donut',\n",
        "    61: 'cake',\n",
        "    62: 'chair',\n",
        "    63: 'couch',\n",
        "    64: 'potted plant',\n",
        "    65: 'bed',\n",
        "    67: 'dining table',\n",
        "    70: 'toilet',\n",
        "    72: 'tv',\n",
        "    73: 'laptop',\n",
        "    74: 'mouse',\n",
        "    75: 'remote',\n",
        "    76: 'keyboard',\n",
        "    77: 'cell phone',\n",
        "    78: 'microwave',\n",
        "    79: 'oven',\n",
        "    80: 'toaster',\n",
        "    81: 'sink',\n",
        "    82: 'refrigerator',\n",
        "    84: 'book',\n",
        "    85: 'clock',\n",
        "    86: 'vase',\n",
        "    87: 'scissors',\n",
        "    88: 'teddy bear',\n",
        "    89: 'hair drier',\n",
        "    90: 'toothbrush',\n",
        "}\n",
        "category_index = {k: {'id': k, 'name': ID_MAPPING[k]} for k in ID_MAPPING}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HngQNsyjXmvF"
      },
      "source": [
        "## Break video into frames and resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oZWLz4xXsyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862ac640-183a-4a26-d362-5d34f194f3c1"
      },
      "source": [
        "# video_name = 'static.webm'\n",
        "video_name = 'part_zibi.mp4'\n",
        "use_drive = True\n",
        "\n",
        "if use_drive: # use videos on our drive\n",
        "  cap = cv2.VideoCapture('/content/drive/MyDrive/videos/{}' .format(video_name))\n",
        "else: \n",
        "  cap = cv2.VideoCapture('{}' .format(video_name))\n",
        "\n",
        "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "i=0\n",
        "\n",
        "%mkdir frames\n",
        "%mkdir frame_res\n",
        "%rm frames/*\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "        break\n",
        "    cv2.imwrite('frames/frame'+str(i)+'.jpg',frame)\n",
        "    i+=1\n",
        "\n",
        "\n",
        "cap.release()\n",
        "\n",
        "check = cv2.imread('frames/frame1.jpg')\n",
        "og_hh, og_ww, layers = check.shape\n",
        "print('Original resolution {} X {}' .format(og_ww, og_hh))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘frames’: File exists\n",
            "mkdir: cannot create directory ‘frame_res’: File exists\n",
            "Original resolution 1920 X 1080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUTS-FHcbI0t"
      },
      "source": [
        "width = og_ww\n",
        "height = og_hh\n",
        "\n",
        "should_resize = True\n",
        "was_resized = False  # keep this False, after resizing happens it turns True\n",
        "resize_factor = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPaafFstbJWE",
        "outputId": "2e896486-318f-48ce-e97d-82ab5ed514e2"
      },
      "source": [
        "if should_resize and not was_resized:\n",
        "  width = width // resize_factor\n",
        "  height = height // resize_factor\n",
        "  newsize = (width, height)\n",
        "  numnum = 0\n",
        "  while os.path.isfile('frames/frame{}.jpg' .format(numnum)):\n",
        "    image = cv2.imread('frames/frame{}.jpg' .format(numnum))\n",
        "    os.remove('frames/frame{}.jpg' .format(numnum))\n",
        "    cv2.imwrite('frames/frame{}.jpg' .format(numnum), cv2.resize(image, newsize))\n",
        "    numnum+=1\n",
        "  was_resized = True  # to ensure the size won't change again\n",
        "  print('After resize - resolution {} X {}' .format(width, height))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After resize - resolution 640 X 360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFDlXcGNgK9i"
      },
      "source": [
        "## DINO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fe5tE45fekH"
      },
      "source": [
        "should_dino = True\n",
        "should_test_dino = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CC4jbZBiRMg"
      },
      "source": [
        "### Downloading Pretrained DINO net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "painted-syndicate"
      },
      "source": [
        "%%capture\n",
        "if should_dino:\n",
        "  resnet50 = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
        "  resnet50.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zLoHcb3iUPO"
      },
      "source": [
        "### Data\n",
        "To fit the clustering algorithm we need data. \n",
        "* place < class1 >'s training images in < data_dir >/train/< class1 >/< class1 > ().jpg\n",
        "* same for < class2 > and for test data\n",
        "\n",
        "You can use the drive if you want!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satellite-supplement"
      },
      "source": [
        "if should_dino:\n",
        "  use_drive_4_dino = True\n",
        "\n",
        "\n",
        "  if use_drive_4_dino:\n",
        "    data_dir = '/content/drive/MyDrive/shoes/dataset_aug'\n",
        "  else:\n",
        "    data_dir = 'C:/Users/tomhe/Desktop/kagglecatsanddogs_3367a/PetImages'\n",
        "\n",
        "  class1_dirname = 'highheel'\n",
        "  class1_filename = 'highheel'\n",
        "  class2_dirname = 'sneaker'\n",
        "  class2_filename = 'sneaker'\n",
        "\n",
        "  dino_was_trained = False  # keep false, once fitted itll turn true and we wouldn't have to run forward passes o the training data again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQgny01ulUKX"
      },
      "source": [
        "### Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC5p8nUykz5K"
      },
      "source": [
        "DINO forward passes on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "endless-million",
        "outputId": "550ab5d1-01b2-4350-c11f-01a2b691915b"
      },
      "source": [
        "if should_dino:\n",
        "  DINO_width = 200\n",
        "  DINO_height = 200\n",
        "  channels = 3\n",
        "  batch_size = 1\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  if not dino_was_trained:\n",
        "    \n",
        "    # for progress\n",
        "    # num_1_4_prog = 10 dataset has bad naming\n",
        "    num_1_4_prog = 1\n",
        "    num_2_4_prog = 1\n",
        "    while os.path.isfile('{}/train/{}/{} ({}).jpg' .format(data_dir, class1_dirname, class1_filename, num_1_4_prog)):\n",
        "      num_1_4_prog += 1\n",
        "    while os.path.isfile('{}/train/{}/{} ({}).jpg' .format(data_dir, class2_dirname, class2_filename, num_2_4_prog)):\n",
        "      num_2_4_prog += 1\n",
        "\n",
        "    # num_1_4_prog -= 10\n",
        "    # num_2_4_prog -= 10\n",
        "\n",
        "    if num_1_4_prog > 500:\n",
        "      num_1_4_prog = 500\n",
        "    if num_2_4_prog > 500:\n",
        "      num_2_4_prog = 500\n",
        "\n",
        "    cls1_features = []\n",
        "    for num_class1 in range(10, num_1_4_prog):\n",
        "      cls1 = cv2.imread('{}/train/{}/{} ({}).jpg' .format(data_dir, class1_dirname, class1_filename, num_class1))\n",
        "      cls1 = cv2.resize(cls1, (DINO_width, DINO_height), interpolation=cv2.INTER_CUBIC)\n",
        "      im = np.expand_dims(cls1, axis=0)\n",
        "      img = torch.from_numpy(im)  # to tensor\n",
        "      images = img.to(device).view(batch_size, channels, DINO_width, DINO_height)  # reshaping \n",
        "\n",
        "      cls1_features.append(resnet50(images.float()).cpu().detach().numpy())\n",
        "      \n",
        "      print('\\r Progress: {}%' .format(int(100*(num_class1)/(num_1_4_prog + num_2_4_prog))), end=\"\")\n",
        "\n",
        "    cls2_features = []\n",
        "    for num_class2 in range(10, num_2_4_prog):\n",
        "      cls2 = cv2.imread('{}/train/{}/{} ({}).jpg' .format(data_dir, class2_dirname, class2_filename, num_class2))\n",
        "      cls2 = cv2.resize(cls2, (DINO_width, DINO_height), interpolation=cv2.INTER_CUBIC)\n",
        "      im = np.expand_dims(cls2, axis=0)\n",
        "      img = torch.from_numpy(im)  # to tensor\n",
        "      images = img.to(device).view(batch_size, channels, DINO_width, DINO_height)  # reshaping \n",
        "\n",
        "      cls2_features.append(resnet50(images.float()).cpu().detach().numpy())\n",
        "\n",
        "      print('\\r Progress: {}%' .format(int(100*(num_class1 + num_class2)/(num_1_4_prog + num_2_4_prog))), end=\"\")\n",
        "\n",
        "    print('\\nTRAIN SET: #class1 {}, #class2 {}'.format(num_1_4_prog, num_2_4_prog))\n",
        "\n",
        "\n",
        "    if not (num_1_4_prog > 0 and num_2_4_prog > 0):\n",
        "      print('Data Unavailable')\n",
        "      raise Exception()\n",
        "\n",
        "    train_features = np.squeeze(np.concatenate((cls1_features, cls2_features)), axis=1)\n",
        "    train_labels = np.concatenate((np.zeros(num_1_4_prog), np.ones(num_2_4_prog)))  # 0 = class1, 1 = class2\n",
        "\n",
        "    # FITTING \n",
        "    K = 3\n",
        "\n",
        "    clfKNN = KNeighborsClassifier(n_neighbors=K)\n",
        "    clfKNN.fit(train_features, train_labels)\n",
        "\n",
        "    clfSVM = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "    clfSVM.fit(train_features, train_labels)\n",
        "\n",
        "    \n",
        "    dino_was_trained = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Progress: 99%\n",
            "TRAIN SET: #class1 58, #class2 230\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-7db2a842ab53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mclfKNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mclfKNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mclfSVM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \"\"\"\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [268, 286]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complete-performance"
      },
      "source": [
        "### Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xc5_oYWk8wq"
      },
      "source": [
        "DINO forward passes on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heavy-police",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceab86d8-5de5-4c2c-a077-f732aa00e3e3"
      },
      "source": [
        "if should_dino:\n",
        "  for ind in range(2):\n",
        "    if ind == 0:\n",
        "      print('KNN\\n')\n",
        "      clf = clfKNN\n",
        "    if ind == 1:\n",
        "      print('SVM\\n')\n",
        "      clf = clfSVM\n",
        "    if should_test_dino:\n",
        "\n",
        "      # for progress\n",
        "      num_1_4_prog = 1\n",
        "      num_2_4_prog = 1\n",
        "      while os.path.isfile('{}/test/{}/{} ({}).jpg' .format(data_dir, class1_dirname, class1_filename, num_1_4_prog)):\n",
        "        num_1_4_prog += 1\n",
        "      while os.path.isfile('{}/test/{}/{} ({}).jpg' .format(data_dir, class2_dirname, class2_filename, num_2_4_prog)):\n",
        "        num_2_4_prog += 1\n",
        "\n",
        "      if num_1_4_prog > 500:\n",
        "        num_1_4_prog = 500\n",
        "      if num_2_4_prog > 500:\n",
        "        num_2_4_prog = 500\n",
        "\n",
        "\n",
        "      cls1_features_test = []\n",
        "      for num_class1_test in range(1, num_1_4_prog):\n",
        "        cls1 = cv2.imread('{}/test/{}/{} ({}).jpg' .format(data_dir, class1_dirname, class1_filename, num_class1_test))\n",
        "        cls1 = cv2.resize(cls1, (DINO_width, DINO_height), interpolation=cv2.INTER_CUBIC)\n",
        "        im = np.expand_dims(cls1, axis=0)\n",
        "\n",
        "        img = torch.from_numpy(im)  # to tensor\n",
        "        images = img.to(device).view(batch_size, channels, DINO_width, DINO_height)  # reshaping \n",
        "\n",
        "        cls1_features_test.append(resnet50(images.float()).cpu().detach().numpy())\n",
        "        \n",
        "        print('\\r Progress: {}%' .format(int(100*(num_class1_test)/(num_1_4_prog))), end=\"\")\n",
        "\n",
        "\n",
        "      cls2_features_test = []\n",
        "      for num_class2_test in range(1, num_2_4_prog):\n",
        "        cls2 = cv2.imread('{}/test/{}/{} ({}).jpg' .format(data_dir, class2_dirname, class2_filename, num_class2_test))\n",
        "        cls2 = cv2.resize(cls2, (DINO_width, DINO_height), interpolation=cv2.INTER_CUBIC)\n",
        "        im = np.expand_dims(cls2, axis=0)\n",
        "\n",
        "        img = torch.from_numpy(im)  # to tensor\n",
        "        images = img.to(device).view(batch_size, channels, DINO_width, DINO_height)  # reshaping \n",
        "\n",
        "        cls2_features_test.append(resnet50(images.float()).cpu().detach().numpy())\n",
        "\n",
        "        print('\\r Progress: {}%' .format(int(100*(num_class2_test)/(num_2_4_prog))), end=\"\")\n",
        "          \n",
        "      print('\\nTEST SET: #class1 {}, #class2 {}'.format(num_1_4_prog, num_2_4_prog))\n",
        "\n",
        "      if not (num_1_4_prog > 0 and num_2_4_prog > 0):\n",
        "        print('Data Unavailable')\n",
        "        raise Exception()\n",
        "\n",
        "      \n",
        "\n",
        "    # 0 = class1, 1 = class2\n",
        "    if should_test_dino:\n",
        "      num_correct_cls1 = 0\n",
        "      for i in range(num_class1_test):\n",
        "          pred = clf.predict(cls1_features_test[i])\n",
        "          if pred == 0:\n",
        "              num_correct_cls1 += 1\n",
        "              \n",
        "      num_correct_cls2 = 0\n",
        "      for i in range(num_class2_test):\n",
        "          pred = clf.predict(cls2_features_test[i])\n",
        "          if pred == 1:\n",
        "              num_correct_cls2 += 1\n",
        "              \n",
        "      ttl_acc = 100*(num_correct_cls2+num_correct_cls1)/(num_class2_test+num_class1_test)\n",
        "      cls2_acc = 100*num_correct_cls2/num_class2_test\n",
        "      cls1_acc = 100*num_correct_cls1/num_class1_test\n",
        "      print('Total Test Acc.  : {}%\\nClass1 Acc. : {}\\nClass2 Acc. : {}'.format(ttl_acc, cls1_acc, cls2_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN\n",
            "\n",
            " Progress: 98%\n",
            "TEST SET: #class1 167, #class2 91\n",
            "Total Test Acc.  : 44.921875%\n",
            "Class1 Acc. : 16.265060240963855\n",
            "Class2 Acc. : 97.77777777777777\n",
            "SVM\n",
            "\n",
            " Progress: 98%\n",
            "TEST SET: #class1 167, #class2 91\n",
            "Total Test Acc.  : 37.109375%\n",
            "Class1 Acc. : 3.0120481927710845\n",
            "Class2 Acc. : 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrmAVMpMlBJm"
      },
      "source": [
        "Cheking Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwrFI4C9Vvam"
      },
      "source": [
        "## Create a Tensorflow session for Mask-RCNN\n",
        "Now let us create a Tensorflow session to run the inference. You can either connect to a TPU or a normal CPU backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0G-tk6wakcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f982e2-0b96-46f9-b829-56f8045602bc"
      },
      "source": [
        "use_tpu = True \n",
        "if use_tpu:\n",
        "  import os\n",
        "  import pprint\n",
        "\n",
        "  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "  session = tf.Session(TPU_ADDRESS, graph=tf.Graph())\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "else:\n",
        "  session = tf.Session(graph=tf.Graph())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU address is grpc://10.88.167.242:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 10308121448120690991),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16405083768195944995),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13598905960868569781),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4410537336732872944),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 40015781556742980),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6152271583902510944),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3485075665373192257),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15413749610181014741),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7186223973127322830),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 58393281689648595),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 4741812338533216682)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtXyXw6EaKRj"
      },
      "source": [
        "## Load the pretrained model for Mask-RCNN\n",
        "Loading the COCO pretrained saved model from the public GCS bucket. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lCL-ZcwaJbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7c097c-79ad-4a78-b597-f70c66a7d27d"
      },
      "source": [
        "saved_model_dir = 'gs://cloud-tpu-checkpoints/mask-rcnn/1555659850' \n",
        "_ = tf.saved_model.loader.load(session, ['serve'], saved_model_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-28-15b92e3b88ec>:2: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "INFO:tensorflow:Restoring parameters from gs://cloud-tpu-checkpoints/mask-rcnn/1555659850/variables/variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXLCyVgZvLDl"
      },
      "source": [
        "##Prepare lobe.ai pretrained Net\n",
        "We trained a net and uploded it to google drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx5qNsCHuTqc"
      },
      "source": [
        "###Extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utt2SIvmxfui"
      },
      "source": [
        "model_name = \"cheat_net_opt\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umgx4k4LHrpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb429b36-f7ab-4f6d-9412-48b2abdad3e7"
      },
      "source": [
        "sh = \"\"\" \n",
        "if [ ! -d \"/content/tf_model\" ]\n",
        "then \n",
        "  mkdir \"/content/tf_model\"\n",
        "  unrar x \"/content/drive/MyDrive/pretrained_models/{}.rar\" \"/content/tf_model/\"\n",
        "fi\n",
        "\"\"\" .format(model_name)\n",
        "with open('script.sh', 'w') as file:\n",
        "  file.write(sh)\n",
        "\n",
        "!bash script.sh "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/pretrained_models/cheat_net_opt.rar\n",
            "\n",
            "Creating    /content/tf_model/cheat_net_opt                           OK\n",
            "Creating    /content/tf_model/cheat_net_opt/example                   OK\n",
            "Extracting  /content/tf_model/cheat_net_opt/example/README.md            \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/tf_model/cheat_net_opt/example/requirements.txt     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/tf_model/cheat_net_opt/example/tf_example.py        \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/tf_model/cheat_net_opt/labels.txt                   \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/tf_model/cheat_net_opt/saved_model.pb               \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/tf_model/cheat_net_opt/signature.json               \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    /content/tf_model/cheat_net_opt/variables                 OK\n",
            "Extracting  /content/tf_model/cheat_net_opt/variables/variables.data-00000-of-00001     \b\b\b\b  4%\b\b\b\b  8%\b\b\b\b 12%\b\b\b\b 17%\b\b\b\b 21%\b\b\b\b 25%\b\b\b\b 30%\b\b\b\b 34%\b\b\b\b 38%\b\b\b\b 42%\b\b\b\b 47%\b\b\b\b 51%\b\b\b\b 55%\b\b\b\b 60%\b\b\b\b 64%\b\b\b\b 68%\b\b\b\b 72%\b\b\b\b 77%\b\b\b\b 81%\b\b\b\b 85%\b\b\b\b 90%\b\b\b\b 94%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Extracting  /content/tf_model/cheat_net_opt/variables/variables.index     \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMhnyDoPnslI"
      },
      "source": [
        "###Functions for LobeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1E_Um4snr3B"
      },
      "source": [
        "#  -------------------------------------------------------------\n",
        "#   Copyright (c) Microsoft Corporation.  All rights reserved.\n",
        "#  -------------------------------------------------------------\n",
        "\"\"\"\n",
        "Skeleton code showing how to load and run the TensorFlow SavedModel export package from Lobe.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "EXPORT_MODEL_VERSION = 1\n",
        "\n",
        "\n",
        "class TFModel:\n",
        "    def __init__(self, model_dir) -> None:\n",
        "        # make sure our exported SavedModel folder exists\n",
        "        self.model_dir = model_dir\n",
        "        with open(os.path.join(model_dir, \"signature.json\"), \"r\") as f:\n",
        "            self.signature = json.load(f)\n",
        "        # self.model_file = \"../\" + self.signature.get(\"filename\")\n",
        "        self.model_file = self.signature.get(\"filename\")\n",
        "        if not os.path.isfile(os.path.join(self.model_dir, self.model_file)):\n",
        "            raise FileNotFoundError(f\"Model file does not exist\")\n",
        "        self.inputs = self.signature.get(\"inputs\")\n",
        "        self.outputs = self.signature.get(\"outputs\")\n",
        "        # placeholder for the tensorflow session\n",
        "        self.session = None\n",
        "\n",
        "        # Look for the version in signature file.\n",
        "        # If it's not found or the doesn't match expected, print a message\n",
        "        version = self.signature.get(\"export_model_version\")\n",
        "        if version is None or version != EXPORT_MODEL_VERSION:\n",
        "            print(\n",
        "                f\"There has been a change to the model format. Please use a model with a signature 'export_model_version' that matches {EXPORT_MODEL_VERSION}.\"\n",
        "            )\n",
        "\n",
        "    def load(self) -> None:\n",
        "        self.cleanup()\n",
        "        # create a new tensorflow session\n",
        "        self.session = tf.compat.v1.Session(graph=tf.Graph())\n",
        "        # load our model into the session\n",
        "        tf.compat.v1.saved_model.loader.load(sess=self.session, tags=self.signature.get(\"tags\"), export_dir=self.model_dir)\n",
        "\n",
        "    def predict(self, image: Image.Image) -> dict:\n",
        "        # load the model if we don't have a session\n",
        "        if self.session is None:\n",
        "            self.load()\n",
        "\n",
        "        image = self.process_image(image, self.inputs.get(\"Image\").get(\"shape\"))\n",
        "        # create the feed dictionary that is the input to the model\n",
        "        # first, add our image to the dictionary (comes from our signature.json file)\n",
        "        feed_dict = {self.inputs[\"Image\"][\"name\"]: [image]}\n",
        "\n",
        "        # list the outputs we want from the model -- these come from our signature.json file\n",
        "        # since we are using dictionaries that could have different orders, make tuples of (key, name) to keep track for putting\n",
        "        # the results back together in a dictionary\n",
        "        fetches = [(key, output[\"name\"]) for key, output in self.outputs.items()]\n",
        "\n",
        "        # run the model! there will be as many outputs from session.run as you have in the fetches list\n",
        "        outputs = self.session.run(fetches=[name for _, name in fetches], feed_dict=feed_dict)\n",
        "        return self.process_output(fetches, outputs)\n",
        "\n",
        "    def process_image(self, image, input_shape) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Given a PIL Image, center square crop and resize to fit the expected model input, and convert from [0,255] to [0,1] values.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        # ensure image type is compatible with model and convert if not\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        # center crop image (you can substitute any other method to make a square image, such as just resizing or padding edges with 0)\n",
        "        if width != height:\n",
        "            square_size = min(width, height)\n",
        "            left = (width - square_size) / 2\n",
        "            top = (height - square_size) / 2\n",
        "            right = (width + square_size) / 2\n",
        "            bottom = (height + square_size) / 2\n",
        "            # Crop the center of the image\n",
        "            image = image.crop((left, top, right, bottom))\n",
        "        # now the image is square, resize it to be the right shape for the model input\n",
        "        input_width, input_height = input_shape[1:3]\n",
        "        if image.width != input_width or image.height != input_height:\n",
        "            image = image.resize((input_width, input_height))\n",
        "\n",
        "        # make 0-1 float instead of 0-255 int (that PIL Image loads by default)\n",
        "        image = np.asarray(image) / 255.0\n",
        "        # format input as model expects\n",
        "        return image.astype(np.float32)\n",
        "\n",
        "    def process_output(self, fetches, outputs) -> dict:\n",
        "        # do a bit of postprocessing\n",
        "        out_keys = [\"label\", \"confidence\"]\n",
        "        results = {}\n",
        "        # since we actually ran on a batch of size 1, index out the items from the returned numpy arrays\n",
        "        for i, (key, _) in enumerate(fetches):\n",
        "            val = outputs[i].tolist()[0]\n",
        "            if isinstance(val, bytes):\n",
        "                val = val.decode()\n",
        "            results[key] = val\n",
        "        confs = results[\"Confidences\"]\n",
        "        labels = self.signature.get(\"classes\").get(\"Label\")\n",
        "        output = [dict(zip(out_keys, group)) for group in zip(labels, confs)]\n",
        "        sorted_output = {\"predictions\": sorted(output, key=lambda k: k[\"confidence\"], reverse=True)}\n",
        "        return sorted_output\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        # close our tensorflow session if one exists\n",
        "        if self.session is not None:\n",
        "            self.session.close()\n",
        "            self.session = None\n",
        "\n",
        "    def __del__(self) -> None:\n",
        "        self.cleanup()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7xnEkpHoid9"
      },
      "source": [
        "### Initializing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvttiEv5oOv7",
        "outputId": "ec2368cb-77a8-44fc-ca4d-5bf37c67b6a4"
      },
      "source": [
        "lobe_net = TFModel('/content/tf_model/{}/' .format(model_name))\n",
        "lobe_net.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/tf_model/cheat_net_opt/variables/variables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tf3pBu91x4G"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csKWyw206URi"
      },
      "source": [
        "def print_group_mat(group_matrix):\n",
        "  print()\n",
        "  for i, data in enumerate(group_matrix):\n",
        "    print(str(i).zfill(2) + \":\" + str(data))\n",
        "\n",
        "\n",
        "def normalize(a):\n",
        "  '''\n",
        "  To compare between colors invariantly of luminance\n",
        "  '''\n",
        "  c = np.zeros(a.shape)\n",
        "  for i in range(len(a)):\n",
        "    b = a[i,:]\n",
        "    m  = float(max(b))\n",
        "    b = b/m\n",
        "    c[i,:] = b\n",
        "  return c\n",
        "\n",
        "\n",
        "def find_groups(Id_vector, group_matrix, dx_thresh, small_changes_th, vel_diff_th):\n",
        "  for i in range(len(Id_vector)):\n",
        "    for j in range(len(Id_vector)):\n",
        "      if (i != j and Id_vector[i][3] == 0 and Id_vector[i][0] > 0 and Id_vector[j][3] == 0 and Id_vector[j][0] > 0):\n",
        "        x_prox = abs(Id_vector[i][0] - Id_vector[j][0]) < dx_thresh  # checks if they are close together\n",
        "        walk_dir = Id_vector[i][11] * Id_vector[j][11] > 0  # if both are same sign, theyre walking in the same direction\n",
        "        small_changes = (abs(Id_vector[i][11]) <= small_changes_th) or (abs(Id_vector[j][11]) <= small_changes_th)\n",
        "        vel_diff = abs(Id_vector[i][9] -  Id_vector[j][9])*3.6  # 3.6 is meter/sec to km/h\n",
        "        if (x_prox and (small_changes or walk_dir) and vel_diff <= vel_diff_th):\n",
        "          group_matrix[i][j] = group_matrix[i][j] + 1\n",
        "        else:\n",
        "          group_matrix[i][j] = 0\n",
        "  return group_matrix\n",
        "\n",
        "\n",
        "def is_in_group_list(groups_list, ID):\n",
        "  for i in range(len(groups_list)):\n",
        "    for j in range(len(groups_list[i])):\n",
        "     if(groups_list[i][j] == ID):\n",
        "       return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def create_groups_list(group_matrix, frames_to_group):\n",
        "\n",
        "  groups_list = [[-1]]\n",
        "  last_index = 0\n",
        "  flag = 0\n",
        "\n",
        "  for i in range(len(group_matrix)):\n",
        "    if(is_in_group_list(groups_list, i) == False):\n",
        "      groups_list[last_index][0] = i\n",
        "      for j in range(len(group_matrix)): \n",
        "        if(is_in_group_list(groups_list, j) == False and group_matrix[i][j] > frames_to_group):\n",
        "          flag = 1\n",
        "          groups_list[last_index].append(j)\n",
        "\n",
        "    if flag == 1:\n",
        "      groups_list.append([-1])\n",
        "      last_index = last_index + 1\n",
        "    else:\n",
        "      groups_list[last_index][0] = -1\n",
        "    flag = 0\n",
        "\n",
        "  return groups_list\n",
        "\n",
        "\n",
        "def findXLimitsOfContours(contours):\n",
        "  min2 = 0\n",
        "  max2 = 0\n",
        "  if len(contours) == 1:\n",
        "    l1 = contours[0]\n",
        "    max1 = 0\n",
        "    min1 = 10000\n",
        "    for cont in range(len(contours)):\n",
        "      if (l1[cont][0][0] >= max1):\n",
        "        max1 = l1[cont][0][0]\n",
        "      if (l1[cont][0][0] <= min1):\n",
        "        min1 = l1[cont][0][0]\n",
        "\n",
        "  elif len(contours) == 2:\n",
        "    l1 = contours[0]\n",
        "    max1 = 0\n",
        "    min1 = 10000\n",
        "    l2 = contours[1]\n",
        "    max2 = 0\n",
        "    min2 = 10000\n",
        "    for cont in range(len(contours)):\n",
        "      if (l1[cont][0][0] >= max1):\n",
        "        max1 = l1[cont][0][0]\n",
        "      if (l1[cont][0][0] <= min1):\n",
        "        min1 = l1[cont][0][0]\n",
        "      if (l2[cont][0][0] >= max2):\n",
        "        max2 = l2[cont][0][0]\n",
        "      if (l2[cont][0][0] <= min2):\n",
        "        min2 = l2[cont][0][0]\n",
        "  return min1, max1, min2, max2\n",
        "\n",
        "\n",
        "\n",
        "def findYLimitsOfContours(contours):\n",
        "  l1 = contours[0]\n",
        "  max = 0\n",
        "  min = 10000\n",
        "  for cont in range(len(contours)):\n",
        "      if (l1[cont][0][1] >= max):\n",
        "        max = l1[cont][0][1]\n",
        "      if (l1[cont][0][1] <= min):\n",
        "        min = l1[cont][0][1]\n",
        "\n",
        "  return min, max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjHuO49maf6R"
      },
      "source": [
        "# Da Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mEld8uRrUT7"
      },
      "source": [
        "# +++++++++++++++ Options +++++++++++++++++++\n",
        "\n",
        "vel_print = False\n",
        "forward_passes_on = True  # should the classifer work (whether its conv net or dino)? also control printing of classification \n",
        "should_print_class_res = False and forward_passes_on  # whether to print the net's results or not (net must be activated)\n",
        "print_flag = False\n",
        "\n",
        "# classification = 'DINO & KNN'\n",
        "# classification = 'DINO & SVM'\n",
        "classification = 'CONV NET'\n",
        "\n",
        "if classification == 'DINO & KNN':\n",
        "  clf = clfKNN\n",
        "elif classification == 'DINO & SVM':\n",
        "  clf = clfSVM\n",
        "\n",
        "  # +++++++++++++++++++++++++++++++++++++++++"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNobHlHo0nSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ce6804-1983-4419-97d1-8911a1eaef61"
      },
      "source": [
        "size = (width,height)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, size)\n",
        "frame_rate = -1\n",
        "print('resolution {} X {}' .format(width, height))\n",
        "\n",
        "# ==================== CONSTANTS =================================\n",
        "\n",
        "first_frame = 0\n",
        "num_frames = 218\n",
        "\n",
        "\n",
        "color_diff_th = 40\n",
        "\n",
        "class_th = 0.4\n",
        "\n",
        "group_distance_thresh = width/6.4 # 100 for part1. how close you have to be in order to be grouped\n",
        "if frame_rate != -1:\n",
        "  frames_to_group = frame_rate/2  # to be in a group togehter you need to be close for 0.5 seconds\n",
        "else:\n",
        "  frames_to_group = 10  # how many frames you have to be close in order to be grouped\n",
        "small_ch_th = width/213.3 # 3 for part1\n",
        "vel_diff_th = width*2/640  # 2 for part1 quick maths\n",
        "\n",
        "close_th = width / 2.56 # 250 for part1\n",
        "\n",
        "frame_stride = 1\n",
        "\n",
        "\n",
        "\n",
        "class_name = 'Shoe' # what's the class' being made, will be printed in the output video\n",
        "\n",
        "class1 = 'highheel'  # what's the class' name on the output of the net\n",
        "class2 = 'sneakers'\n",
        "\n",
        "class1_4_print = 'HH'  # what's the class' name on the output video\n",
        "class2_4_print = 'SNKR'\n",
        "# ================================================================"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resolution 640 X 360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qENTOHzZcaWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bfca0f-48fc-4403-b81b-0e7de266b60c"
      },
      "source": [
        "'''\n",
        "////////////////////// ID VECTOR DICTIONARY////////////////////////////\n",
        "0 - x_avg\n",
        "1 - x_length\n",
        "2 - y_height\n",
        "3 - how many frames in a row this person wasn't detected\n",
        "4 - BBOX's maximal y\n",
        "5 - mask area in pixels\n",
        "6 - BBOX's maximal x\n",
        "7 - avg color in upper body\n",
        "8 - avg color in bottom body\n",
        "9 - velocity (after adjusments) \n",
        "10 - how many frames in total this person was seen in (doesn't have to be in a row)\n",
        "11 - dx - how many pixels this person moved\n",
        "12 - leg_dist\n",
        "13 - steps - cooldown and frame counter (after X=10 frames you can make another step by closing your legs for Y=3 frames in a row)\n",
        "14 - step counter\n",
        "15 - classification score - grows if class 1, diminishes if class 2\n",
        "\n",
        "///////////////////////////////////////////////////////////////////////\n",
        "'''\n",
        "\n",
        "start_time = time.time() # to check FPS\n",
        "\n",
        "!rm for_lobe_net/*\n",
        "!mkdir for_lobe_net\n",
        "\n",
        "!rm bboxes_4_print/*\n",
        "!mkdir bboxes_4_print\n",
        "\n",
        "\n",
        "\n",
        "i = first_frame\n",
        "total_counter = 0\n",
        "color_diff = 0\n",
        "number_of_id = 30\n",
        "number_of_features = 17\n",
        "\n",
        "\n",
        "Id_vector = [[0]*number_of_features]\n",
        "last_avail_idx = 0\n",
        "last_id = copy.deepcopy(Id_vector)\n",
        "\n",
        "\n",
        "\n",
        "group_matrix = [0]*number_of_id\n",
        "for k in range(len(group_matrix)):\n",
        "  group_matrix[k] = [0]*number_of_id\n",
        "\n",
        "\n",
        "while ((i - first_frame) <= num_frames):  # frames loop\n",
        "\n",
        "  if print_flag:\n",
        "    print('============================ FRAME {} ==================================\\n\\n\\n\\n\\n' .format(i))\n",
        "  # initializing variables\n",
        "  flag_exist = 0\n",
        "\n",
        "  #print(\"iter num:\" ,i)\n",
        "  image_path = 'frames/frame'+str(i)+'.jpg'\n",
        "  i += frame_stride\n",
        "  with open(image_path, 'rb') as f:\n",
        "    np_image_string = np.array([f.read()])\n",
        "\n",
        "\n",
        "\n",
        "  orig_img = cv2.imread(image_path) \n",
        "\n",
        "  image = Image.open(image_path)\n",
        "\n",
        "  # width, height = image.size\n",
        "  np_image = np.array(image.getdata()).reshape(height, width, 3).astype(np.uint8)\n",
        "\n",
        "  # running the network\n",
        "  num_detections, detection_boxes, detection_classes, detection_scores, detection_masks, image_info = session.run(\n",
        "      ['NumDetections:0', 'DetectionBoxes:0', 'DetectionClasses:0', 'DetectionScores:0', 'DetectionMasks:0', 'ImageInfo:0'],\n",
        "      feed_dict={'Placeholder:0': np_image_string})\n",
        "\n",
        "  # extracting info from the output\n",
        "  num_detections = np.squeeze(num_detections.astype(np.int32), axis=(0,))\n",
        "  detection_boxes = np.squeeze(detection_boxes * image_info[0, 2], axis=(0,))[0:num_detections]\n",
        "  detection_scores = np.squeeze(detection_scores, axis=(0,))[0:num_detections]\n",
        "  detection_classes = np.squeeze(detection_classes.astype(np.int32), axis=(0,))[0:num_detections]\n",
        "  detection_scores[detection_classes != 1] = 0\n",
        "  instance_masks = np.squeeze(detection_masks, axis=(0,))[0:num_detections]\n",
        "  ymin, xmin, ymax, xmax = np.split(detection_boxes, 4, axis=-1)\n",
        "  processed_boxes = np.concatenate([xmin, ymin, xmax - xmin, ymax - ymin], axis=-1)\n",
        "  segmentations = coco_metric.generate_segmentation_from_masks(instance_masks, processed_boxes, height, width)\n",
        "\n",
        "  max_boxes_to_draw = 10 \n",
        "  min_score_thresh = 0.99\n",
        "\n",
        "  # producing visualization\n",
        "  image_with_detections = visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      np_image,\n",
        "      detection_boxes,\n",
        "      detection_classes,\n",
        "      detection_scores,\n",
        "      category_index,\n",
        "      instance_masks=segmentations,\n",
        "      use_normalized_coordinates=False,\n",
        "      max_boxes_to_draw=max_boxes_to_draw,\n",
        "      min_score_thresh=min_score_thresh)\n",
        "  output_image_path = 'frame_res/frame results'+str(i)+'.jpg'\n",
        "  Image.fromarray(image_with_detections.astype(np.uint8)).save(output_image_path)\n",
        "\n",
        "  # bbox coordinates for all people detected in frame\n",
        "  ymin_new = np.zeros(max_boxes_to_draw)\n",
        "  xmin_new = np.zeros(max_boxes_to_draw)\n",
        "  ymax_new = np.zeros(max_boxes_to_draw)\n",
        "  xmax_new = np.zeros(max_boxes_to_draw)\n",
        "  image = cv2.imread(output_image_path)\n",
        "  index_person = detection_classes == 1 \n",
        "  index_thresh = detection_scores >= min_score_thresh\n",
        " \n",
        "  people_in_frame = 0\n",
        "  for j in range(len(index_thresh)):\n",
        "    if index_person[j] and index_thresh[j] and people_in_frame < max_boxes_to_draw:\n",
        "      ymin_new[people_in_frame] = ymin[j]\n",
        "      xmin_new[people_in_frame] = xmin[j]\n",
        "      ymax_new[people_in_frame] = ymax[j]\n",
        "      xmax_new[people_in_frame] = xmax[j]\n",
        "      people_in_frame+=1\n",
        "\n",
        "  last_id = copy.deepcopy(Id_vector)\n",
        "\n",
        "\n",
        "\n",
        "  # -------------------- TRACKING ---------------------------\n",
        "\n",
        "  for j in range(people_in_frame):  # for all people in frame\n",
        "    if xmin_new[j] > 0 or xmax_new[j] < width:  # if in frame\n",
        "      \n",
        "      x_avg = (int(xmax_new[j])+int(xmin_new[j]))/2\n",
        "      x_length = int(xmax_new[j])-int(xmin_new[j])\n",
        "      y_height = int(ymax_new[j])-int(ymin_new[j])\n",
        "      y_avg = int(((ymax_new[j])+(ymin_new[j]))/2)\n",
        "      y_coordinate = int((int(ymin_new[j])*0.75+int(ymax_new[j])*0.25))\n",
        "      \n",
        "\n",
        "      # color calculations\n",
        "      up = segmentations[index_person,:,:]\n",
        "      up[:,y_avg:,:] = 0\n",
        "      up[:,int(ymin_new[j]):int(ymin_new[j])+20,:] = 0\n",
        "      up_person_avg = int(np.average(orig_img[up[j,:,:] == 1,]))\n",
        "\n",
        "      down = segmentations[index_person,:,:]\n",
        "      down[:,0:y_avg,:] = 0\n",
        "      down_person_avg = int(np.average(orig_img[down[j,:,:] == 1,]))\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      checkss = segmentations[index_person,:,:]\n",
        "      mask_area = int((np.sum(checkss[j,:,:])))\n",
        "      \n",
        "\n",
        "      min_color_diff = 300  # arbitrary large value\n",
        "      min_index = -1  # initial illegal value\n",
        "\n",
        "      isnt_first_frame = (i != first_frame)\n",
        "      if isnt_first_frame:\n",
        "        # calculating match scores\n",
        "        '''\n",
        "        color_diff (takes properties of two people and produces scalar) is our target \n",
        "        function. \n",
        "        We are looking for the person 'k' in Id_vector with the least color_diff from \n",
        "        the person 'j' that was detected in this frame and coresponds with the variables\n",
        "        <up_person_avg> and <down_person_avg>.\n",
        "        '''\n",
        "\n",
        "        if print_flag:  # produce masked bbox fot j_th person for printing only\n",
        "            mbb = np.copy(orig_img)\n",
        "            mask = [segmentations[index_person,:,:], segmentations[index_person,:,:], segmentations[index_person,:,:]]\n",
        "            mbb[mask == 1] = 0\n",
        "            masked_bbox_j = mbb[int(ymin_new[j]):int(ymax_new[j]), int(xmin_new[j]):int(xmax_new[j])]\n",
        "\n",
        "        for k in range(len(last_id)): \n",
        "          if  last_id[k][3] >= 0 and last_id[k] != [0]*number_of_features:\n",
        "            color_diff =  abs(last_id[k][7] - up_person_avg) + abs(last_id[k][8] - down_person_avg)\n",
        "            is_close_enough = abs(last_id[k][0] - x_avg) <= close_th\n",
        "            is_similar_size = (max((last_id[k][5]/mask_area),(mask_area/last_id[k][5])) <= 2)\n",
        "\n",
        "            # side_margin = 25\n",
        "            # is_in_sides = (xmin_new[j] <= side_margin or xmax_new[j] >= width - side_margin)\n",
        "\n",
        "\n",
        "            if print_flag:\n",
        "              print('\\n\\n\\n-----------------------------------------------------------------\\n')\n",
        "              print('Current (j_th) person\\nUpper Avg. = {}, Bottom Avg. = {}' .format(up_person_avg, down_person_avg))\n",
        "              cv2_imshow(masked_bbox_j)\n",
        "              print('\\n\\nCompared to (k_th) person\\nUpper Avg. = {}, Bottom Avg. = {}' .format(last_id[k][7], last_id[k][8]))\n",
        "              cv2_imshow(cv2.imread('bboxes_4_print/mask_id_{}.jpg' .format(k)))\n",
        "\n",
        "              print('Match Score: {}' .format(color_diff))\n",
        "              print('\\n-----------------------------------------------------------------\\n\\n\\n')\n",
        "            if (color_diff <= min_color_diff and is_close_enough and is_similar_size):\n",
        "              min_color_diff = color_diff\n",
        "              min_index = k\n",
        "\n",
        "        k = min_index\n",
        "        color_diff = min_color_diff\n",
        "        if print_flag: print('\\n\\nb\\Best Match: ID {}, Score {}\\n\\n\\n' .format(min_index, min_color_diff))\n",
        "        #x_max_change = abs(last_id[min_index][6] - int(xmax_new[j]))\n",
        "        \n",
        "\n",
        "\n",
        "        # ========================= EXISTING PERSON ==============================\n",
        "        '''\n",
        "        if the best match is good enough' this is an existing person\n",
        "        (hasn't died) we've already seen\n",
        "        '''\n",
        "        if ((color_diff <= color_diff_th and last_id[min_index][3] >= 0) or (color_diff <= 80 and last_id[min_index][3] >= 0 and (abs(y_height-last_id[min_index][2]) <= height/24))):\n",
        "          flag_exist = 1  # this person does exist!\n",
        "\n",
        "          if print_flag:\n",
        "            cv2.imwrite('bboxes_4_print/mask_id_{}.jpg' .format(min_index), masked_bbox_j)\n",
        "\n",
        "          dx = abs(x_avg - last_id[min_index][0])/(width/5)\n",
        "          vel = 30*(dx/(last_id[min_index][3]+1))\n",
        "\n",
        "          # ----------------- updating Id_vector -----------------------\n",
        "          Id_vector[min_index][10] = Id_vector[min_index][10] + 1\n",
        "          if (int(ymax_new[j]) >= 320):\n",
        "            Id_vector[min_index][9] = ((Id_vector[min_index][9]*Id_vector[min_index][10] + vel*0.6)/(Id_vector[min_index][10]+1))\n",
        "          elif (int(ymax_new[j]) <= 275):\n",
        "            Id_vector[min_index][9] = ((Id_vector[min_index][9]*Id_vector[min_index][10] + vel*1.3)/(Id_vector[min_index][10]+1))\n",
        "          else:\n",
        "            Id_vector[min_index][9] = ((Id_vector[min_index][9]*Id_vector[min_index][10] + vel)/(Id_vector[min_index][10]+1))\n",
        "          Id_vector[min_index][11] = x_avg - Id_vector[min_index][0]\n",
        "          Id_vector[min_index][0] = x_avg\n",
        "          Id_vector[min_index][1] = x_length\n",
        "          Id_vector[min_index][2] = y_height\n",
        "          Id_vector[min_index][4] = int(ymax_new[j])\n",
        "          Id_vector[min_index][5] = mask_area\n",
        "          Id_vector[min_index][6] = int(xmax_new[j])\n",
        "          Id_vector[min_index][7] = up_person_avg\n",
        "          Id_vector[min_index][8] = down_person_avg\n",
        "          # ------------------------------------------------------------\n",
        "          # -------------------- step counter --------------------------\n",
        "          down_f_s = segmentations[index_person,:,:]\n",
        "          down_f_s[j,0:int(ymax_new[j])-int(0.3*y_height),:] = 0\n",
        "          yossi = down_f_s[j,:,:]\n",
        "          kernel = np.ones((5,5), np.uint8)\n",
        "          img_erosion = cv2.erode(yossi, kernel, iterations=1)\n",
        "          yossi = cv2.dilate(img_erosion, kernel, iterations=1)\n",
        "            \n",
        "          contours, hierarchy = cv2.findContours(yossi, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "          if (len(contours) != 1 and len(contours) != 2):\n",
        "            continue\n",
        "          elif (len(contours) == 1):\n",
        "            cnt2 = 0 # how many adjecent rows had two blobs\n",
        "            max_blob_dist = 0\n",
        "            actually_two_legs = False\n",
        "            for y in range(Id_vector[min_index][4] - Id_vector[min_index][2], Id_vector[min_index][4]):\n",
        "              prev = 0\n",
        "              cnt = 0\n",
        "              x1to0 = 0 \n",
        "              x0to1 = 0\n",
        "              for x in range(Id_vector[min_index][6] - Id_vector[min_index][1], Id_vector[min_index][6]):\n",
        "                if yossi[y,x] == 1 and prev == 0:  # from black to white \n",
        "                  cnt += 1   \n",
        "                  if cnt == 2:\n",
        "                    x0to1 = x  # saving the x cordinate of the start of the second blob\n",
        "\n",
        "                elif yossi[y,x] == 0 and prev == 1 and cnt == 1:  # from white to black\n",
        "                    x1to0 = x \n",
        "                prev = yossi[y,x]\n",
        "              if cnt == 2:\n",
        "                cnt2 += 1\n",
        "                if x0to1 - x1to0 > max_blob_dist:\n",
        "                  max_blob_dist = x0to1 - x1to0\n",
        "                if cnt2 == 5:\n",
        "                  actually_two_legs = True\n",
        "              else:\n",
        "                cnt2 = 0\n",
        "                \n",
        "            if actually_two_legs:\n",
        "              leg_dist = max_blob_dist\n",
        "            else:\n",
        "              leg_dist = 0\n",
        "          elif (len(contours) == 2):\n",
        "            l1 = contours[0]\n",
        "            max1 = 0\n",
        "            min1 = 1000\n",
        "            l2 = contours[1]\n",
        "            max2 = 0\n",
        "            min2 = 1000\n",
        "            for cont in range(len(contours)):\n",
        "              if (l1[cont][0][0] >= max1):\n",
        "                max1 = l1[cont][0][0]\n",
        "              if (l1[cont][0][0] <= min1):\n",
        "                min1 = l1[cont][0][0]\n",
        "              if (l2[cont][0][0] >= max2):\n",
        "                max2 = l2[cont][0][0]\n",
        "              if (l2[cont][0][0] <= min2):\n",
        "                min2 = l2[cont][0][0]\n",
        "            if (min1 > min2):\n",
        "              leg_dist = min1 - max2\n",
        "            else:\n",
        "              leg_dist = min2 - max1\n",
        "          if (Id_vector[min_index][13] < 0):\n",
        "            Id_vector[min_index][13] += 1\n",
        "          else:\n",
        "            if leg_dist < Id_vector[min_index][12] and leg_dist != 0: \n",
        "              Id_vector[min_index][13] += 1\n",
        "            else:\n",
        "              if abs(leg_dist - Id_vector[min_index][12] > 5):\n",
        "                Id_vector[min_index][13] = 0\n",
        "              else:\n",
        "                Id_vector[min_index][13] = max(0, Id_vector[min_index][13] - 1)\n",
        "          #if ((leg_dist == 0 and Id_vector[min_index][12] <= 50 and Id_vector[min_index][12] > 0) or (leg_dist <= 50 and leg_dist > 0 and Id_vector[min_index][12] == 0)):\n",
        "          if (Id_vector[min_index][13] == 3):\n",
        "            Id_vector[min_index][14] += 1\n",
        "            Id_vector[min_index][13] = -10\n",
        "\n",
        "          Id_vector[min_index][12] = leg_dist\n",
        "\n",
        "          # ------------------------------------------------------------\n",
        "\n",
        "          if forward_passes_on:\n",
        "            # ----------------- forward pass -----------------------------\n",
        "            # shlomo is the masked BBOX\n",
        "            x_start = Id_vector[min_index][6] - Id_vector[min_index][1]\n",
        "            x_end = Id_vector[min_index][6]\n",
        "            y_start = int(Id_vector[min_index][4] - 1*Id_vector[min_index][2])\n",
        "            y_end = Id_vector[min_index][4]\n",
        "            shlomo = copy.deepcopy(orig_img)\n",
        "            # shlomo[checkss[j,:,:] == 0] = 0  # make it masked\n",
        "            height_shlomo = (int(ymax_new[j]) - int(ymin_new[j]))\n",
        "            shlomo = shlomo[int(ymin_new[j]+0.85*height_shlomo):int(ymax_new[j]),int(xmin_new[j]):int(xmax_new[j]),:]\n",
        "            \n",
        "            if classification == 'CONV NET':\n",
        "              # outputs = lobe_net.predict(Image.fromarray(np.uint8(shlomo*255)))\n",
        "              outputs = lobe_net.predict(Image.fromarray(np.uint8(shlomo)))\n",
        "              winner = outputs[\"predictions\"][0][\"label\"]\n",
        "              # print(f\"Predicted: {outputs}\")\n",
        "\n",
        "            else:  # DINO and some clustering method\n",
        "              sh_DINO = cv2.resize(shlomo, (DINO_width, DINO_height), interpolation=cv2.INTER_CUBIC)\n",
        "              sh_DINO = np.expand_dims(sh_DINO, axis=0)\n",
        "              sh_DINO_tensor = torch.from_numpy(sh_DINO)  # to tensor\n",
        "              sh_DINO_tensors = sh_DINO_tensor.to(device).view(batch_size, channels, DINO_width, DINO_height)  # reshaping \n",
        "\n",
        "              shlomo_feature_map = resnet50(sh_DINO_tensors.float()).cpu().detach().numpy()\n",
        "              # SelectStrongFeatures?\n",
        "              pred = clf.predict(shlomo_feature_map)\n",
        "              if pred == 0:\n",
        "                winner = class1\n",
        "              elif pred == 1:\n",
        "                winner = class2\n",
        "            # cv2.imwrite('/content/for_lobe_net/bbox_frame{}_id{}.jpg' .format(i, min_index), shlomo)\n",
        "            if should_print_class_res:\n",
        "              cv2_imshow(shlomo)\n",
        "              print(f\"Predicted: {winner}\")\n",
        "            if winner == class1:\n",
        "              Id_vector[min_index][15] += 1\n",
        "            elif winner == class2:\n",
        "              Id_vector[min_index][15] -= 1\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "        # =================== END OF EXISTING PERSON =============================\n",
        "\n",
        "        # ============================ NEW PERSON ================================\n",
        "        if (flag_exist == 0):\n",
        "          if print_flag:  # produce masked bbox fot j_th person for printing only\n",
        "            cv2.imwrite('bboxes_4_print/mask_id_{}.jpg' .format(last_avail_idx), masked_bbox_j)\n",
        "\n",
        "          Id_vector[last_avail_idx][0] = x_avg\n",
        "          Id_vector[last_avail_idx][1] = x_length\n",
        "          Id_vector[last_avail_idx][2] = y_height\n",
        "          Id_vector[last_avail_idx][4] = int(ymax_new[j])\n",
        "          Id_vector[last_avail_idx][5] = mask_area\n",
        "          Id_vector[last_avail_idx][6] = int(xmax_new[j])\n",
        "          Id_vector[last_avail_idx][7] = up_person_avg\n",
        "          Id_vector[last_avail_idx][8] = down_person_avg\n",
        "          Id_vector[last_avail_idx][10] = 1\n",
        "          Id_vector[last_avail_idx][13] = 0\n",
        "          Id_vector[last_avail_idx][14] = 0\n",
        "          Id_vector[last_avail_idx][16] = 0 # frame counter\n",
        "\n",
        "\n",
        "          if forward_passes_on:\n",
        "            # ----------------- forward pass -----------------------------\n",
        "            # shlomo is the masked BBOX\n",
        "            x_start = Id_vector[last_avail_idx][6] - Id_vector[last_avail_idx][1]\n",
        "            x_end = Id_vector[last_avail_idx][6]\n",
        "            y_start = int(Id_vector[last_avail_idx][4] - 1*Id_vector[last_avail_idx][2])\n",
        "            y_end = Id_vector[last_avail_idx][4]\n",
        "\n",
        "            shlomo = copy.deepcopy(orig_img)\n",
        "            # shlomo[checkss[j,:,:] == 0] = 0  # make it masked\n",
        "            height_shlomo = (int(ymax_new[j]) - int(ymin_new[j]))\n",
        "            shlomo = shlomo[int(ymin_new[j]+0.85*height_shlomo):int(ymax_new[j]),int(xmin_new[j]):int(xmax_new[j]),:]\n",
        "            if classification == 'CONV NET':\n",
        "              # outputs = lobe_net.predict(Image.fromarray(np.uint8(shlomo*255)))\n",
        "              outputs = lobe_net.predict(Image.fromarray(np.uint8(shlomo)))\n",
        "              winner = outputs[\"predictions\"][0][\"label\"]\n",
        "              # print(f\"Predicted: {outputs}\")\n",
        "\n",
        "            else:  # DINO and some clustering method\n",
        "              sh_DINO = cv2.resize(shlomo, (DINO_width, DINO_height), interpolation=cv2.INTER_CUBIC)\n",
        "              sh_DINO = np.expand_dims(sh_DINO, axis=0)\n",
        "              sh_DINO_tensor = torch.from_numpy(sh_DINO)  # to tensor\n",
        "              sh_DINO_tensors = sh_DINO_tensor.to(device).view(batch_size, channels, DINO_width, DINO_height)  # reshaping \n",
        "              shlomo_feature_map = resnet50(sh_DINO_tensors.float()).cpu().detach().numpy()\n",
        "              # SelectStrongFeatures?\n",
        "              pred = clf.predict(shlomo_feature_map)\n",
        "              if pred == 0:\n",
        "                winner = class1\n",
        "              elif pred == 1:\n",
        "                winner = class2\n",
        "\n",
        "            # cv2.imwrite('/content/for_lobe_net/bbox_frame{}_id{}.jpg' .format(i, min_index), shlomo)\n",
        "            if should_print_class_res:\n",
        "              cv2_imshow(shlomo)\n",
        "              print(f\"Predicted: {winner}\")\n",
        "            if winner == class1:\n",
        "              Id_vector[last_avail_idx][15] += 1\n",
        "            elif winner == class2:\n",
        "              Id_vector[last_avail_idx][15] -= 1\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "          Id_vector.append([0]*number_of_features)\n",
        "          last_id.append([0]*number_of_features)  \n",
        "          last_avail_idx += 1\n",
        "          \n",
        "              \n",
        "      else: # it is the first frame, shouldn't run match checks\n",
        "        print('first Frame!')\n",
        "        if print_flag:  # produce masked bbox fot j_th person for printing only\n",
        "            mbb = np.copy(orig_img)\n",
        "            mask = [segmentations[index_person,:,:], segmentations[index_person,:,:], segmentations[index_person,:,:]]\n",
        "            mbb[mask == 1] = 0\n",
        "            masked_bbox_j = mbb[int(ymin_new[j]):int(ymax_new[j]), int(xmin_new[j]):int(xmax_new[j])]\n",
        "            cv2.imwrite('bboxes_4_print/mask_id_{}.jpg' .format(last_avail_idx), masked_bbox_j)\n",
        "        Id_vector[last_avail_idx][0] = x_avg\n",
        "        Id_vector[last_avail_idx][1] = x_length\n",
        "        Id_vector[last_avail_idx][2] = y_height\n",
        "        Id_vector[last_avail_idx][4] = int(ymax_new[j])\n",
        "        Id_vector[last_avail_idx][5] = mask_area\n",
        "        Id_vector[last_avail_idx][6] = int(xmax_new[j])\n",
        "        Id_vector[last_avail_idx][7] = up_person_avg\n",
        "        Id_vector[last_avail_idx][8] = down_person_avg\n",
        "        Id_vector[last_avail_idx][10] = 1\n",
        "        Id_vector[last_avail_idx][13] = 0\n",
        "        Id_vector[last_avail_idx][14] = 0\n",
        "\n",
        "        if forward_passes_on:\n",
        "            # ----------------- forward pass -----------------------------\n",
        "            # shlomo is the masked BBOX\n",
        "            x_start = Id_vector[last_avail_idx][6] - Id_vector[last_avail_idx][1]\n",
        "            x_end = Id_vector[last_avail_idx][6]\n",
        "            y_start = int(Id_vector[last_avail_idx][4] - 1*Id_vector[last_avail_idx][2])\n",
        "            y_end = Id_vector[last_avail_idx][4]\n",
        "\n",
        "            shlomo = copy.deepcopy(orig_img)\n",
        "            # shlomo[checkss[j,:,:] == 0] = 0  # make it masked\n",
        "            height_shlomo = (int(ymax_new[j]) - int(ymin_new[j]))\n",
        "            shlomo = shlomo[int(ymin_new[j]+0.85*height_shlomo):int(ymax_new[j]),int(xmin_new[j]):int(xmax_new[j]),:]\n",
        "            if classification == 'CONV NET':\n",
        "              # outputs = lobe_net.predict(Image.fromarray(np.uint8(shlomo*255)))\n",
        "              outputs = lobe_net.predict(Image.fromarray(np.uint8(shlomo)))\n",
        "              winner = outputs[\"predictions\"][0][\"label\"]\n",
        "              # print(f\"Predicted: {outputs}\")\n",
        "\n",
        "            else:  # DINO and some clustering method\n",
        "              sh_DINO = cv2.resize(shlomo, (DINO_width, DINO_height), interpolation=cv2.INTER_CUBIC)\n",
        "              sh_DINO = np.expand_dims(sh_DINO, axis=0)\n",
        "              sh_DINO_tensor = torch.from_numpy(sh_DINO)  # to tensor\n",
        "              sh_DINO_tensors = sh_DINO_tensor.to(device).view(batch_size, channels, DINO_width, DINO_height)  # reshaping \n",
        "\n",
        "              shlomo_feature_map = resnet50(sh_DINO_tensors.float()).cpu().detach().numpy()\n",
        "              # SelectStrongFeatures?\n",
        "              pred = clf.predict(shlomo_feature_map)\n",
        "              if pred == 0:\n",
        "                winner = class1\n",
        "              elif pred == 1:\n",
        "                winner = class2\n",
        "\n",
        "            # cv2.imwrite('/content/for_lobe_net/bbox_frame{}_id{}.jpg' .format(i, min_index), shlomo)\n",
        "            if should_print_class_res:\n",
        "              cv2_imshow(shlomo)\n",
        "              print(f\"Predicted: {winner}\")\n",
        "            if winner == class1:\n",
        "              Id_vector[last_avail_idx][15] += 1\n",
        "            elif winner == class2:\n",
        "              Id_vector[last_avail_idx][15] -= 1\n",
        "            # ------------------------------------------------------------\n",
        "\n",
        "        Id_vector.append([0]*number_of_features)\n",
        "        last_id.append([0]*number_of_features)  \n",
        "        last_avail_idx += 1\n",
        "      \n",
        "\n",
        "      re = copy.deepcopy(orig_img)\n",
        "      re[checkss[j,:,:] == 0] = 0\n",
        "      re = re[int(ymin_new[j]):int(ymax_new[j]),int(xmin_new[j]):int(xmax_new[j]),:]\n",
        "      # cv2.imwrite('aut_people/out'+str(i)+'_'+str(j)+'.jpg',re)\n",
        "\n",
        "  for k in range(len(Id_vector)):\n",
        "        if last_id[k][:] == Id_vector[k][:] and Id_vector[k][0] > 0 and Id_vector[k][3] >= 0:\n",
        "          Id_vector[k][3] += 1\n",
        "        elif Id_vector[k][3] >= 0:\n",
        "          Id_vector[k][3] = 0\n",
        "        if (Id_vector[k][3] >= 30 and (Id_vector[k][0] <= 40 or Id_vector[k][0] >= width - 40)):\n",
        "          Id_vector[k][3] = -1\n",
        "        elif (Id_vector[k][3] >= 80):\n",
        "          Id_vector[k][3] = -1\n",
        "        \n",
        "\n",
        "  group_matrix = find_groups(Id_vector, group_matrix, group_distance_thresh, small_ch_th, vel_diff_th)\n",
        "\n",
        "  groups_list = create_groups_list(group_matrix, frames_to_group)\n",
        "\n",
        "  # PRINTS!\n",
        "  for k in range(len(Id_vector)):\n",
        "    if (Id_vector[k][0] > 0):\n",
        "      total_counter += 1\n",
        "      if Id_vector[k][3] == 0:\n",
        "        cv2.putText(image, 'ID:' + str(k+1) + '.',( max(1,(int(Id_vector[k][0])-10)), Id_vector[k][4]), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
        "        cv2.putText(image, 'steps:' + str(Id_vector[k][14]) + '.',( max(1,(int(Id_vector[k][0])-10)), min(height,Id_vector[k][4]+10)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
        "        if Id_vector[k][9] != 0 and vel_print:\n",
        "          cv2.putText(image, 'V:' + str(int(round(Id_vector[k][9]*3.6))) + 'KM/H',( max(1,(int(Id_vector[k][0])-10)), Id_vector[k][4]-40), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
        "        if forward_passes_on and Id_vector[k][10] > 0 and Id_vector[k][15]/Id_vector[k][10] >= class_th:\n",
        "          cv2.putText(image, '{}: {}'.format(class_name, class1_4_print),( max(1,(int(Id_vector[k][0])-10)), Id_vector[k][4]-60), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
        "        elif forward_passes_on:\n",
        "          cv2.putText(image, '{}: {}'.format(class_name, class2_4_print),( max(1,(int(Id_vector[k][0])-10)), Id_vector[k][4]-60), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)\n",
        "\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  for group in groups_list:\n",
        "    max_x = 0\n",
        "    min_x = 100000\n",
        "    min_y = 100000\n",
        "    max_y = 0\n",
        "    if (len(group) <= 1):\n",
        "      continue\n",
        "    if group[0] != -1:\n",
        "      for person in group:\n",
        "        if (Id_vector[person][3] == 0):  # don't print if missing from frame\n",
        "          if (min(Id_vector[person][6], Id_vector[person][6]-Id_vector[person][1]) < min_x):\n",
        "            min_x = min(Id_vector[person][6], Id_vector[person][6]-Id_vector[person][1])\n",
        "          if (max(Id_vector[person][6], Id_vector[person][6]-Id_vector[person][1]) > max_x):\n",
        "            max_x = max(Id_vector[person][6], Id_vector[person][6]-Id_vector[person][1])\n",
        "          if (min(Id_vector[person][4], Id_vector[person][4]-Id_vector[person][2]) < min_y):\n",
        "            min_y = min(Id_vector[person][4], Id_vector[person][4]-Id_vector[person][2])\n",
        "          if (max(Id_vector[person][4], Id_vector[person][4]-Id_vector[person][2]) > max_y):\n",
        "            max_y = max(Id_vector[person][4], Id_vector[person][4]-Id_vector[person][2])\n",
        "      if (min_x != 100000 and min_y != 100000):\n",
        "        cv2.rectangle(image, (int(max_x), int(min_y)), (int(min_x), int(max_y)), (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "  cv2.putText(image, 'number of people counted:' + str(total_counter) + '.', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
        "  total_counter = 0\n",
        "  if print_flag: cv2_imshow(image)\n",
        "  out.write(image)\n",
        "  #print(\" iter num {}, Id Vector {}\".format(i,Id_vector))\n",
        "  print(\"\\r iter num {}, Id Vector {}\".format(i,Id_vector), end=\"\")\n",
        "  # print(Id_vector[2])\n",
        "  if print_flag: print('=======================================================================\\n\\n\\n\\n\\n' .format(i))\n",
        "\n",
        "out.release()\n",
        "\n",
        "duration = time.time() - start_time\n",
        "avg_fps = num_frames / duration\n",
        "print('\\nRunning took {} [sec]\\nAVG FPS: {} [frames/sec]' .format(duration, avg_fps))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'for_lobe_net/*': No such file or directory\n",
            "mkdir: cannot create directory ‘for_lobe_net’: File exists\n",
            "rm: cannot remove 'bboxes_4_print/*': No such file or directory\n",
            "mkdir: cannot create directory ‘bboxes_4_print’: File exists\n",
            " iter num 219, Id Vector [[183.0, 50, 162, 0, 265, 4480, 208, 132, 90, 0.37152270735981296, 213, -1.0, 28, 0, 7, 154, 0], [340.0, 72, 211, 0, 287, 7638, 376, 69, 62, 0.6917348130841121, 213, 1.5, 33, -2, 6, 35, 0], [134.0, 58, 177, 0, 278, 5215, 163, 91, 109, 0.30957512315270946, 202, -2.0, 22, 0, 8, 180, 0], [388.5, 83, 199, 0, 283, 5728, 430, 100, 83, 2.5053442158830297, 217, -2.0, 37, 0, 4, 37, 0], [258.0, 96, 230, 0, 307, 8860, 306, 83, 85, 0.40234374999999983, 191, -4.5, 28, 1, 6, 161, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Running took 422.43919491767883 [sec]\n",
            "AVG FPS: 0.5160505999981415 [frames/sec]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78T9eKSmQhb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcba005-0a57-4723-8539-f86c2fb555cb"
      },
      "source": [
        "# out.release()\n",
        "from google.colab import files\n",
        "files.download(\"/content/output.mp4\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ef8b1515-6fe5-4dba-b8f9-db02a334691a\", \"output.mp4\", 5708633)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVwVC0YNByPM"
      },
      "source": [
        "# %rm frame_res/*\n",
        "# %rm frames/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4z40Sday9iR"
      },
      "source": [
        "%%capture\n",
        "!zip -r /content/file.zip /content/for_lobe_net\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJqU56RdgjV"
      },
      "source": [
        "# Visualize the detection results\n",
        "Time to check out the result!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuDqEsMzdHfF"
      },
      "source": [
        "max_boxes_to_draw = 30  \n",
        "min_score_thresh = 0.5   \n",
        "\n",
        "image_with_detections = visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    np_image,\n",
        "    detection_boxes,\n",
        "    detection_classes,\n",
        "    detection_scores,\n",
        "    category_index,\n",
        "    instance_masks=segmentations,\n",
        "    use_normalized_coordinates=False,\n",
        "    max_boxes_to_draw=max_boxes_to_draw,\n",
        "    min_score_thresh=min_score_thresh)\n",
        "output_image_path = 'test_results.jpg'\n",
        "Image.fromarray(image_with_detections.astype(np.uint8)).save(output_image_path)\n",
        "display.display(display.Image(output_image_path, width=1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwJpRS9_mNdo"
      },
      "source": [
        "#Using LobeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFgbF_1wmSo5"
      },
      "source": [
        "should_show = True\n",
        "\n",
        "# preds = [[]]*num_frames \n",
        " \n",
        "for i in range(num_frames):\n",
        "  frame_num = i + first_frame + 1\n",
        "  flag = True\n",
        "  id_num = 0\n",
        "  while flag:\n",
        "    im_path = 'for_lobe_net/bbox_frame{}_id{}.jpg' .format(frame_num, id_num)\n",
        "    if os.path.isfile(im_path):\n",
        "      photo = Image.open(im_path)\n",
        "      outputs = lobe_net.predict(photo)\n",
        "      # print(f\"Predicted: {outputs}\")\n",
        "      winner = outputs[\"predictions\"][0][\"label\"]\n",
        "      # preds[i].append(winner)\n",
        "      \n",
        "      if should_show:\n",
        "        photo2see = cv2.imread('for_lobe_net/bbox_frame{}_id{}.jpg' .format(frame_num, id_num))\n",
        "        print('-------------\\nPrediction: {}, ID:{}, Frame: {}' .format(winner, id_num, frame_num))\n",
        "        cv2_imshow(photo2see)\n",
        "        print('---------------------\\n\\n\\n\\n\\n')\n",
        "      id_num += 1\n",
        "    else:\n",
        "      flag = False\n",
        "    \n",
        "# print('#frame  |  #ID  |  prediction')\n",
        "# for i in range(len(preds)):\n",
        "#   for id_idx in range(len(preds[i])):\n",
        "#     print(\"{}  ,  {}  ,  {}\" .format(i + first_frame + 1, id_idx, preds[i][id_idx] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiN9WS654y9x"
      },
      "source": [
        "Evaluating model on test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhO7wU5v46Fi"
      },
      "source": [
        "men_total_cnt = 0\n",
        "men_corret_cnt = 0\n",
        "women_total_cnt = 0\n",
        "women_corret_cnt = 0\n",
        "\n",
        "for i in range(100):\n",
        "  im_path = '/content/drive/MyDrive/people/test_set_part2/man/man ({}).png' .format(i)\n",
        "  if os.path.isfile(im_path):\n",
        "    photo = Image.open(im_path)\n",
        "    outputs = lobe_net.predict(photo)\n",
        "    # print(f\"Predicted: {outputs}\")\n",
        "    winner = outputs[\"predictions\"][0][\"label\"]\n",
        "    men_total_cnt += 1\n",
        "    if winner == 'man': \n",
        "      men_corret_cnt += 1\n",
        "\n",
        "  im_path = '/content/drive/MyDrive/people/test_set_part2/woman/woman ({}).png' .format(i)\n",
        "  if os.path.isfile(im_path):\n",
        "    photo = Image.open(im_path)\n",
        "    outputs = lobe_net.predict(photo)\n",
        "    # print(f\"Predicted: {outputs}\")\n",
        "    winner = outputs[\"predictions\"][0][\"label\"]\n",
        "    women_total_cnt += 1\n",
        "    if winner == 'woman': \n",
        "      women_corret_cnt += 1\n",
        "\n",
        "ttl_acc = 100*(women_corret_cnt + men_corret_cnt)/(women_total_cnt + men_total_cnt)\n",
        "women_acc = 100*women_corret_cnt/women_total_cnt\n",
        "men_acc = 100*men_corret_cnt/men_total_cnt\n",
        "print('All Photos: {} ({} woman {} man)'.format(men_total_cnt + women_total_cnt, women_total_cnt, men_total_cnt))\n",
        "print('Overall Acc.: {}% ({} woman {} man)'.format(ttl_acc, women_acc, men_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}